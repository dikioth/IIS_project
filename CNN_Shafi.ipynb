{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load library\n",
    "import ipywidgets as widgets\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ipywebrtc import CameraStream, ImageRecorder, VideoRecorder\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "import sklearn as sk\n",
    "from sklearn.datasets import make_classification\n",
    "import scikitplot as skplt\n",
    "from sklearn.model_selection import train_test_split # Helps with organizing data for training\n",
    "from sklearn.metrics import confusion_matrix # Helps present results as a confusion-matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters to tune and Library installation needed\n",
    "1. install pandas on windows using command: pip install pandas\n",
    "2. Install xlrd to read excell file using pandas using command : pip install xlrd\n",
    "Here we shall use all the parameters and later all will be combined code\n",
    "\n",
    "Note: for video as input... the directory you are working inside that we expect there will be directory called videos and inside that corresponding directory names and video should be there.(example working dir/Videos/102)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "training_data_path = 'dataset/subsystem_2/Dataset_subsystem_2.csv'\n",
    "test_data_path = 'dataset/subsystem_1/Dataset_Subsystem_1.xlsx'\n",
    "#Read the directory name\n",
    "video_subdirecory_name = \"videos\"\n",
    "#directory_names= [\"102\",\"159\",\"294\",\"441\",\"564\",\"576\",\"609\",\"666\",\"711\",\"723\"]\n",
    "directory_names= [\"723\"]\n",
    "conditions = [\"open_palm\",\"open_dorsal\",\"fist_palm\",\"fist_dorsal\",\"three_fingers_palm\",\"three_fingers_dorsal\"]\n",
    "#conditions = [\"open_palm\"]\n",
    "video_file_extensions = [\".mp4\",\".webm\"]\n",
    "\n",
    "number_of_epoch = 50\n",
    "#Note Input of training is all land mark and output column name is output_label which classified fro 0 to 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process data from training and test file \n",
    "Here all feature columns (landmark position) are of type Int64 and out put column name renamed as output_label\n",
    "Note: output_column has been assigned 0 to 5 assigned class\n",
    "Class names: \n",
    "0 = open palm,\n",
    "1 = fist palm,\n",
    "2 = three_fingers palm,\n",
    "3 = open dorsal,\n",
    "4 = fist dorsal,\n",
    "5 = three_fingers dorsal,\n",
    "\n",
    "Another column gesture is assigned 0 and 1. This we shall use on convolution model to identify if the hand is palm side or dorsal side\n",
    "0 = palm side\n",
    "1 = dorsal side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process data function. Look iteration skipped because iteration reduce performance\n",
    "def processedData(data):\n",
    "    #process output data by labeling multiclass 0 to 5 instead of one hot encoding\n",
    "    processing_data = data\n",
    "    processing_data.loc[processing_data.camera_facing_side.isin(['open']) & processing_data.gesture.isin(['palm']), 'camera_facing_side'] = 0\n",
    "    processing_data.loc[processing_data.camera_facing_side.isin(['fist']) & processing_data.gesture.isin(['palm']), 'camera_facing_side'] = 1\n",
    "    processing_data.loc[processing_data.camera_facing_side.isin(['three_fingers']) & processing_data.gesture.isin(['palm']), 'camera_facing_side'] = 2\n",
    "    processing_data.loc[processing_data.camera_facing_side.isin(['open']) & processing_data.gesture.isin(['dorsal']), 'camera_facing_side'] = 3\n",
    "    processing_data.loc[processing_data.camera_facing_side.isin(['fist']) & processing_data.gesture.isin(['dorsal']), 'camera_facing_side'] = 4\n",
    "    processing_data.loc[processing_data.camera_facing_side.isin(['three_fingers']) & processing_data.gesture.isin(['dorsal']), 'camera_facing_side'] = 5\n",
    "\n",
    "    # this will be used later to classify if hand is open or dorsal side fro raw image\n",
    "    processing_data.loc[processing_data.gesture == 'palm', 'gesture'] = 0\n",
    "    processing_data.loc[processing_data.gesture == 'dorsal', 'gesture'] = 1\n",
    "    #Convert all column to integer type\n",
    "    processing_data = processing_data.astype({\"camera_facing_side\": np.int64, \"gesture\": np.int64}) \n",
    "    #rename output column name as label\n",
    "    processing_data=processing_data.rename(columns = {'camera_facing_side':'output_label'})\n",
    "    return processing_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data\n",
    "Here you can drop columns and get the columns for input and as well as y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Training Data\n",
    "#Load Training Dataset dropping unnecessary columns\n",
    "training_data = pd.read_csv(training_data_path, sep=\",\" , na_values='?' , dtype={'camera_facing_side': str,'gesture': str}).dropna().reset_index().drop(columns=['ID','frame'])\n",
    "processed_train_data= processedData(training_data)\n",
    "#X_train = processed_train_data.drop(columns=['output_label','gesture'])\n",
    "#Y_train = processed_train_data['output_label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62219, 83)\n"
     ]
    }
   ],
   "source": [
    "print(processed_train_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train prediction Model (Choose NN or Random forest?) not implemented\n",
    "Train the model here and find the accuracy.Train set and validation set learned as 80% - 20%\n",
    "use code below to get train and test set(Coppied from validation)\n",
    "        trainI = np.random.choice(processed_train_data.shape[0], size = train_size, replace=False)\n",
    "        trainIndex = songClassifyData.index.isin(trainI)\n",
    "        \n",
    "        train_dataset = processed_train_data.iloc[trainIndex]\n",
    "        #rest of index will be used as validation dataset for the model\n",
    "        validation_dataset = processed_train_data.iloc[~trainIndex]\n",
    "        \n",
    "        X_train= train_dataset.drop(columns=['output_label','gesture'])\n",
    "        Y_train = train_dataset['output_label']\n",
    "        \n",
    "        X_validation= validation_dataset.drop(columns=['output_label','gesture'])\n",
    "        Y_validation = validation_dataset['output_label']\n",
    "\n",
    "Note: not implemented here: learn the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset = processed_train_data.iloc[trainIndex]\n",
    "#rest of index will be used as validation dataset for the model\n",
    "#validation_dataset = processed_train_data.iloc[~trainIndex]\n",
    "\n",
    "X_train= processed_train_data.drop(columns=['output_label','gesture'])\n",
    "Y_train = processed_train_data['output_label']\n",
    "\n",
    "X_validation= validation_dataset.drop(columns=['output_label','gesture'])\n",
    "Y_validation = validation_dataset['output_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of        index  palm_root_x  palm_root_y  palm_thumb_1_x  palm_thumb_1_y  \\\n",
      "0          0          494          295             412             266   \n",
      "1          1          493          294             410             266   \n",
      "2          2          493          294             409             267   \n",
      "3          3          492          294             407             267   \n",
      "4          4          491          294             405             267   \n",
      "...      ...          ...          ...             ...             ...   \n",
      "62214  62214            0            0               0               0   \n",
      "62215  62215            0            0               0               0   \n",
      "62216  62216            0            0               0               0   \n",
      "62217  62217            0            0               0               0   \n",
      "62218  62218            0            0               0               0   \n",
      "\n",
      "       palm_thumb_2_x  palm_thumb_2_y  palm_thumb_3_x  palm_thumb_3_y  \\\n",
      "0                 376             243             346             225   \n",
      "1                 375             243             345             225   \n",
      "2                 374             243             344             225   \n",
      "3                 373             243             343             225   \n",
      "4                 373             243             342             225   \n",
      "...               ...             ...             ...             ...   \n",
      "62214               0               0               0               0   \n",
      "62215               0               0               0               0   \n",
      "62216               0               0               0               0   \n",
      "62217               0               0               0               0   \n",
      "62218               0               0               0               0   \n",
      "\n",
      "       palm_index_1_x  ...  dorsal_ring_4_x  dorsal_ring_4_y  \\\n",
      "0                 434  ...                0                0   \n",
      "1                 433  ...                0                0   \n",
      "2                 432  ...                0                0   \n",
      "3                 431  ...                0                0   \n",
      "4                 430  ...                0                0   \n",
      "...               ...  ...              ...              ...   \n",
      "62214               0  ...                0                0   \n",
      "62215               0  ...                0                0   \n",
      "62216               0  ...                0                0   \n",
      "62217               0  ...                0                0   \n",
      "62218               0  ...                0                0   \n",
      "\n",
      "       dorsal_pinky_1_x  dorsal_pinky_1_y  dorsal_pinky_2_x  dorsal_pinky_2_y  \\\n",
      "0                     0                 0                 0                 0   \n",
      "1                     0                 0                 0                 0   \n",
      "2                     0                 0                 0                 0   \n",
      "3                     0                 0                 0                 0   \n",
      "4                     0                 0                 0                 0   \n",
      "...                 ...               ...               ...               ...   \n",
      "62214               304               363               287               329   \n",
      "62215               305               363               287               330   \n",
      "62216               306               364               287               331   \n",
      "62217               307               365               287               332   \n",
      "62218               308               366               287               332   \n",
      "\n",
      "       dorsal_pinky_3_x  dorsal_pinky_3_y  dorsal_pinky_4_x  dorsal_pinky_4_y  \n",
      "0                     0                 0                 0                 0  \n",
      "1                     0                 0                 0                 0  \n",
      "2                     0                 0                 0                 0  \n",
      "3                     0                 0                 0                 0  \n",
      "4                     0                 0                 0                 0  \n",
      "...                 ...               ...               ...               ...  \n",
      "62214                 0                 0                 0                 0  \n",
      "62215                 0                 0                 0                 0  \n",
      "62216                 0                 0                 0                 0  \n",
      "62217                 0                 0                 0                 0  \n",
      "62218                 0                 0                 0                 0  \n",
      "\n",
      "[62219 rows x 81 columns]>\n"
     ]
    }
   ],
   "source": [
    "print(X_train.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network with dense layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of a new class to fix a bug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this snippet will dissapear at some point in the future\n",
    "# it exists because I found two bugs in KerasClassifier while writing\n",
    "# this notebook. \n",
    "# Issues are raised in the tensorflow repo and this should be fixed soon(TM)\n",
    "\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier \n",
    "\n",
    "class KerasClassifier_Patched(KerasClassifier):\n",
    "    # bugfix: classifier doesn't declare that it is a classifier\n",
    "    # in the Scikit learn API\n",
    "    _estimator_type = \"classifier\"\n",
    "    \n",
    "    # bugfix: the current wrapper does not work with HotOne encoded\n",
    "    # labels\n",
    "    # this is only a fix in the specific case of this notebook,\n",
    "    # not a general one\n",
    "    def score(self, x, y, **kwargs):\n",
    "        _, accuracy = self.model.evaluate(x,y, verbose=0, **kwargs)\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "def setupModel(): #very similar to the one from Lab 2\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Input(shape=(80)))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(6))\n",
    "    model.add(tf.keras.layers.Softmax())\n",
    "    model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=1e-4),\n",
    "                 loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "                 metrics=[tf.keras.metrics.CategoricalAccuracy()])\n",
    "    return model\n",
    "\n",
    "final_model = KerasClassifier_Patched(build_fn=setupModel,\n",
    "                                      epochs=3,\n",
    "                               batch_size=50,\n",
    "                               verbose=1)\n",
    "\n",
    "#Model training\n",
    "#final_model.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-fold Cross validation\n",
    "Here learn the model and calculate accuracy average. \n",
    "=> not implemented here: learn the model and find the accuracy for each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 49775 samples\n",
      "Epoch 1/3\n",
      "49775/49775 [==============================] - 3s 69us/sample - loss: 4.6914 - categorical_accuracy: 0.9623\n",
      "Epoch 2/3\n",
      "49775/49775 [==============================] - 3s 65us/sample - loss: 0.0481 - categorical_accuracy: 0.9967\n",
      "Epoch 3/3\n",
      "49775/49775 [==============================] - 3s 63us/sample - loss: 0.0025 - categorical_accuracy: 0.9995\n",
      "(12444, 80)\n",
      "(12444,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You are passing a target array of shape (12444, 1) while using as loss `categorical_crossentropy`. `categorical_crossentropy` expects targets to be binary matrices (1s and 0s) of shape (samples, classes). If your targets are integer classes, you can convert them to the expected format via:\n```\nfrom keras.utils import to_categorical\ny_binary = to_categorical(y_int)\n```\n\nAlternatively, you can use the loss function `sparse_categorical_crossentropy` instead, which does expect integer targets.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-e2d71c4dca33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m#learn the model and find the accuracy for each epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0maccuracy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_validation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_validation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-94797289ba02>\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# not a general one\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    928\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m   def predict(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, model, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m         use_multiprocessing=use_multiprocessing, **kwargs)\n\u001b[0m\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m   def predict(self, model, x, batch_size=None, verbose=0, steps=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[0;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    424\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    427\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m       \u001b[0muse_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_samples\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[0;34m(model, mode, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    644\u001b[0m     \u001b[0mstandardize_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m     x, y, sample_weights = standardize(\n\u001b[0;32m--> 646\u001b[0;31m         x, y, sample_weight=sample_weights)\n\u001b[0m\u001b[1;32m    647\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0madapter_cls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mdata_adapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mListsOfScalarsDataAdapter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0mstandardize_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstandardize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2381\u001b[0m         \u001b[0mis_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2382\u001b[0m         \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2383\u001b[0;31m         batch_size=batch_size)\n\u001b[0m\u001b[1;32m   2384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2385\u001b[0m   def _standardize_tensors(self, x, y, sample_weight, run_eagerly, dict_inputs,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_tensors\u001b[0;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[1;32m   2487\u001b[0m           \u001b[0;31m# Additional checks to avoid users mistakenly using improper loss fns.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2488\u001b[0m           training_utils.check_loss_and_target_compatibility(\n\u001b[0;32m-> 2489\u001b[0;31m               y, self._feed_loss_fns, feed_output_shapes)\n\u001b[0m\u001b[1;32m   2490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2491\u001b[0m       sample_weights, _, _ = training_utils.handle_partial_sample_weights(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mcheck_loss_and_target_compatibility\u001b[0;34m(targets, loss_fns, output_shapes)\u001b[0m\n\u001b[1;32m    782\u001b[0m         raise ValueError('You are passing a target array of shape ' +\n\u001b[1;32m    783\u001b[0m                          \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m                          \u001b[0;34m' while using as loss `categorical_crossentropy`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    785\u001b[0m                          \u001b[0;34m'`categorical_crossentropy` expects '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m                          \u001b[0;34m'targets to be binary matrices (1s and 0s) '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: You are passing a target array of shape (12444, 1) while using as loss `categorical_crossentropy`. `categorical_crossentropy` expects targets to be binary matrices (1s and 0s) of shape (samples, classes). If your targets are integer classes, you can convert them to the expected format via:\n```\nfrom keras.utils import to_categorical\ny_binary = to_categorical(y_int)\n```\n\nAlternatively, you can use the loss function `sparse_categorical_crossentropy` instead, which does expect integer targets."
     ]
    }
   ],
   "source": [
    "# n-fold cross validation\n",
    "np.random.seed(1)\n",
    "# Divide train and validation set as 80% and 20%\n",
    "number_of_rows = len(processed_train_data.index)\n",
    "train_size = int ( (number_of_rows * 8) / 10)\n",
    "accuracy = np.zeros(number_of_epoch)\n",
    "final_model.fit(X_train,Y_train)\n",
    "for i in range(number_of_epoch):\n",
    "        #get 80% randomly choice of trainindex\n",
    "        trainI = np.random.choice(processed_train_data.shape[0], size = train_size, replace=False)\n",
    "        trainIndex = processed_train_data.index.isin(trainI)\n",
    "        \n",
    "        train_dataset = processed_train_data.iloc[trainIndex]\n",
    "        #rest of index will be used as validation dataset for the model\n",
    "        validation_dataset = processed_train_data.iloc[~trainIndex]\n",
    "        \n",
    "        X_train= train_dataset.drop(columns=['output_label','gesture','index'])\n",
    "        Y_train = train_dataset['output_label']\n",
    "        \n",
    "        X_validation= validation_dataset.drop(columns=['output_label','gesture','index'])\n",
    "        Y_validation = validation_dataset['output_label']\n",
    "        print(X_validation.shape)\n",
    "        print(Y_validation.shape)\n",
    "        \n",
    "        #learn the model and find the accuracy for each epoch\n",
    "        accuracy[i] = final_model.score(X_validation,Y_validation)\n",
    "\n",
    "# find the average of accuracy            \n",
    "average_accuracy = np.mean(accuracy, axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test dataset \n",
    "read the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load test dataset input\n",
    "test_data = pd.read_excel(test_data_path, index_col=0, comment='#').dropna().reset_index() \n",
    "processed_test_data= processedData(test_data).drop(columns=['ID'])\n",
    "#X_test = processed_test_data.drop(columns=['output_label','gesture','source','frame'])\n",
    "##Y_test = processed_test_data['output_label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function write predicted output, landmark to the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to write output class on video frame and draw circle and line\n",
    "def WriteImageframe(image_frame, img_x_test_input_array, output_class):\n",
    "        cv2.putText(image_frame, \"Predicted \" + str(output_class), (5, 50),cv2.FONT_HERSHEY_SIMPLEX, 2, (128,0,0), 2)\n",
    "\n",
    "        #draw circle and line\n",
    "        xposroot = img_x_test_input_array[\"palm_root_x\"].iloc[ 0 ] \n",
    "        yposroot = img_x_test_input_array[\"palm_root_y\"].iloc[ 0 ] \n",
    "        root_connection_number = 2\n",
    "        thumb_iteration = 2\n",
    "        other_finger_iteration = 12\n",
    "        other_finger_sub_iteration = 3\n",
    "        for indexval in range(0,80,2):\n",
    "            xpos = img_x_test_input_array.iloc[ 0 , indexval ] \n",
    "            ypos = img_x_test_input_array.iloc[ 0 , indexval + 1 ] \n",
    "            cv2.circle(image_frame,(xpos, ypos), 3, (128,0,50),5)\n",
    "\n",
    "            if thumb_iteration == 0 :\n",
    "                if other_finger_sub_iteration == 0:\n",
    "                    other_finger_sub_iteration = 3\n",
    "                    if other_finger_iteration == 0:\n",
    "                        thumb_iteration = 2\n",
    "                        other_finger_iteration = 12\n",
    "                        root_connection_number += 2\n",
    "                        xposroot = img_x_test_input_array[\"dorsal_root_x\"].iloc[ 0 ] \n",
    "                        yposroot = img_x_test_input_array[\"dorsal_root_y\"].iloc[ 0 ] \n",
    "            if thumb_iteration > 0 and  indexval >= root_connection_number :\n",
    "                xposth1 = img_x_test_input_array.iloc[ 0 , indexval ] \n",
    "                yposth1 = img_x_test_input_array.iloc[ 0 , indexval + 1 ] \n",
    "                xposth2 = img_x_test_input_array.iloc[ 0 , indexval + 2] \n",
    "                yposth2 = img_x_test_input_array.iloc[ 0 , indexval + 3] \n",
    "                thumb_iteration -= 1\n",
    "                if thumb_iteration == 0 :\n",
    "                    root_connection_number += 6\n",
    "                if (xposth1 != 0 and yposth1 != 0):\n",
    "                        if (xposth2 != 0 and yposth2 != 0):\n",
    "                            cv2.line(image_frame, (xposth1, yposth1), (xposth2, yposth2), (128, 0, 10), 1)\n",
    "                if indexval == root_connection_number:\n",
    "                    if (xposroot != 0 and yposroot != 0):\n",
    "                        if (xposth1 != 0 and yposth1 != 0):\n",
    "                            cv2.line(image_frame, (xposroot, yposroot), (xposth1, yposth1), (128, 0, 10), 1)           \n",
    "            else:\n",
    "                if thumb_iteration <= 0 and other_finger_iteration > 0 and other_finger_sub_iteration > 0 and indexval >= root_connection_number:\n",
    "                    xposth1 = img_x_test_input_array.iloc[ 0 , indexval ] \n",
    "                    yposth1 = img_x_test_input_array.iloc[ 0 , indexval + 1 ] \n",
    "                    xposth2 = img_x_test_input_array.iloc[ 0 , indexval + 2 ] \n",
    "                    yposth2 = img_x_test_input_array.iloc[ 0 , indexval + 3 ] \n",
    "                    other_finger_iteration -= 1\n",
    "                    other_finger_sub_iteration -= 1\n",
    "                    if other_finger_sub_iteration == 0 :\n",
    "                        root_connection_number += 8\n",
    "                    if (xposth1 != 0 and yposth1 != 0):\n",
    "                            if (xposth2 != 0 and yposth2 != 0):\n",
    "                                cv2.line(image_frame, (xposth1, yposth1), (xposth2, yposth2), (128, 0, 10), 1)\n",
    "                    if indexval == root_connection_number:\n",
    "                        if (xposroot != 0 and yposroot != 0):\n",
    "                            if (xposth1 != 0 and yposth1 != 0):\n",
    "                                cv2.line(image_frame, (xposroot, yposroot), (xposth1, yposth1), (128, 0, 10), 1)  \n",
    "        return image_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the input video for each frame :  predict and write output to the image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subsystem 1 : Landmark extraction. Get input landmark from image and feed it to the model\n",
    "import os\n",
    "current_working_directory = os.getcwd()\n",
    "for directory_name in directory_names:\n",
    "    for video_name in conditions:\n",
    "        for extension in video_file_extensions:\n",
    "            video_path =directory_name + \"\\\\\" + video_name \n",
    "            #print(current_working_directory + \"\\\\\" + video_path + extension)\n",
    "            input_video = cv2.VideoCapture(r\"\" + video_subdirecory_name + \"\\\\\" + video_path + extension)\n",
    "            output_file_name = r\"\" + video_subdirecory_name + \"\\\\\" + video_path  + \"_predicted.mp4\"\n",
    "            backend = cv2.CAP_ANY\n",
    "            fourcc_code = cv2.VideoWriter_fourcc(*\"H264\")\n",
    "            fps = 24\n",
    "            frame_size = (640, 480)\n",
    "            output_video = cv2.VideoWriter(output_file_name, backend, fourcc_code, fps, frame_size)\n",
    "\n",
    "            ret, frame = input_video.read()\n",
    "            counter = 0\n",
    "            while ret:\n",
    "                ret, frame = input_video.read()\n",
    "                if not ret:\n",
    "                    continue\n",
    "                print(counter)\n",
    "                data_frame = processed_test_data.loc[processed_test_data.source.isin([directory_name + \"/\" + video_name  + extension]) & processed_test_data.frame.isin([counter])]\n",
    "                \n",
    "                #Input landmark - feed this to prediction model\n",
    "                X_test_input = data_frame.drop(columns=['output_label','gesture','source','frame'])\n",
    "                if X_test_input.empty == False :\n",
    "                    #put output class to the image from predicted model. Now put as on the data\n",
    "                    #X_test_output = data_frame['output_label']\n",
    "                    #output_frame = WriteImageframe( frame, X_test_input ,str(X_test_output.iloc[ 0 ] ))\n",
    "                    print(final_model.predict(X_test_input.drop(columns=['index'])))\n",
    "                else:\n",
    "                    output_frame = frame\n",
    "                output_video.write(output_frame)\n",
    "                counter += 1\n",
    "            input_video.release()\n",
    "            output_video.release()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specialization : Not yet implemented(Do not run) : Choose one specialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specialization 1: CNN to identify hand and predict output(from given video file as input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN Classification model generation\n",
    "# Import of keras model and hidden layers for  convolutional network\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "# Construction of model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (5, 5), activation='relu', input_shape=(120, 320, 1))) \n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu')) \n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "# Configures the model for training\n",
    "model.compile(optimizer='adam', # Optimization routine, which tells the computer how to adjust the parameter values to minimize the loss function.\n",
    "              loss='sparse_categorical_crossentropy', # Loss function, which tells us how bad our predictions are.\n",
    "              metrics=['accuracy']) # List of metrics to be evaluated by the model during training and testing.\n",
    "# Trains the model for a given number of epochs (iterations on a dataset) and validates it.\n",
    "model.fit(X_train, Y_train, epochs=5, batch_size=64, verbose=2, validation_data=(X_test, Y_test))\n",
    "# Save entire model to a HDF5 file\n",
    "model.save('handrecognition_model.h5')\n",
    "test_loss, test_acc = model.evaluate(X_test, Y_test)\n",
    "\n",
    "print('Test accuracy: {:2.2f}%'.format(test_acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specialization 1: Automatically detect hand landmark and provide as input to Model (Random forest or NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take live video as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write predicted output to csv \n",
    "This will have 2 column: frame number and output\n",
    "Read this file for the robot part"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
