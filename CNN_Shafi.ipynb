{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load library\n",
    "import ipywidgets as widgets\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ipywebrtc import CameraStream, ImageRecorder, VideoRecorder\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "import sklearn as sk\n",
    "from sklearn.datasets import make_classification\n",
    "import scikitplot as skplt\n",
    "from sklearn.model_selection import train_test_split # Helps with organizing data for training\n",
    "from sklearn.metrics import confusion_matrix # Helps present results as a confusion-matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters to tune and Library installation needed\n",
    "1. install pandas on windows using command: pip install pandas\n",
    "2. Install xlrd to read excell file using pandas using command : pip install xlrd\n",
    "Here we shall use all the parameters and later all will be combined code\n",
    "\n",
    "Note: for video as input... the directore you are working inside that we expect there will be directory called videos and inside that corresponding directory names and video should be there.(example working dir/Videos/102)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "training_data_path = 'Dataset Subsystem 2.csv'\n",
    "test_data_path = 'Video Data.xlsx'\n",
    "#Read the directory name\n",
    "video_subdirecory_name = \"videos\"\n",
    "#directory_names= [\"102\",\"159\",\"294\",\"441\",\"564\",\"576\",\"609\",\"666\",\"711\",\"723\"]\n",
    "directory_names= [\"723\"]\n",
    "conditions = [\"open_palm\",\"open_dorsal\",\"fist_palm\",\"fist_dorsal\",\"three_fingers_palm\",\"three_fingers_dorsal\"]\n",
    "#conditions = [\"open_palm\"]\n",
    "video_file_extensions = [\".mp4\",\".webm\"]\n",
    "\n",
    "number_of_epoch = 50\n",
    "#Note Input of training is all land mark and output column name is output_label which classified fro 0 to 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process data from training and test file \n",
    "Here all feature columns (landmark position) are of type Int64 and out put column name renamed as output_label\n",
    "Note: output_column has been assigned 0 to 5 assigned class\n",
    "Class names: \n",
    "0 = open palm\n",
    "1 = fist palm\n",
    "2 = three_fingers palm\n",
    "3 = open dorsal\n",
    "4 = fist dorsal\n",
    "5 = three_fingers dorsal\n",
    "\n",
    "Another column gesture is assigned 0 and 1. This we shall use on convolution model to identify if the hand is palm side or dorsal side\n",
    "0 = palm side\n",
    "1 = dorsal side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process data function. Look iteration skipped because iteration reduce performance\n",
    "def processedData(data):\n",
    "    #process output data by labeling multiclass 0 to 5 instead of one hot encoding\n",
    "    processing_data = data\n",
    "    processing_data.loc[processing_data.camera_facing_side.isin(['open']) & processing_data.gesture.isin(['palm']), 'camera_facing_side'] = 0\n",
    "    processing_data.loc[processing_data.camera_facing_side.isin(['fist']) & processing_data.gesture.isin(['palm']), 'camera_facing_side'] = 1\n",
    "    processing_data.loc[processing_data.camera_facing_side.isin(['three_fingers']) & processing_data.gesture.isin(['palm']), 'camera_facing_side'] = 2\n",
    "    processing_data.loc[processing_data.camera_facing_side.isin(['open']) & processing_data.gesture.isin(['dorsal']), 'camera_facing_side'] = 3\n",
    "    processing_data.loc[processing_data.camera_facing_side.isin(['fist']) & processing_data.gesture.isin(['dorsal']), 'camera_facing_side'] = 4\n",
    "    processing_data.loc[processing_data.camera_facing_side.isin(['three_fingers']) & processing_data.gesture.isin(['dorsal']), 'camera_facing_side'] = 5\n",
    "\n",
    "    # this will be used later to classify if hand is open or dorsal side fro raw image\n",
    "    processing_data.loc[processing_data.gesture == 'palm', 'gesture'] = 0\n",
    "    processing_data.loc[processing_data.gesture == 'dorsal', 'gesture'] = 1\n",
    "    #Convert all column to integer type\n",
    "    processing_data = processing_data.astype({\"camera_facing_side\": np.int64, \"gesture\": np.int64}) \n",
    "    #rename output column name as label\n",
    "    processing_data=processing_data.rename(columns = {'camera_facing_side':'output_label'})\n",
    "    return processing_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data\n",
    "Here you can drop columns and get the columns for input and as well as y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Training Data\n",
    "#Load Training Dataset dropping unnecessary columns\n",
    "training_data = pd.read_csv(training_data_path, sep=\",\" , na_values='?' , dtype={'camera_facing_side': str,'gesture': str}).dropna().reset_index().drop(columns=['ID','frame'])\n",
    "processed_train_data= processedData(training_data)\n",
    "#X_train = processed_train_data.drop(columns=['output_label','gesture'])\n",
    "#Y_train = processed_train_data['output_label']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train prediction Model (Choose NN or Random forest?) not implemented\n",
    "Train the model here and find the accuracy.Train set and validation set learned as 80% - 20%\n",
    "use code below to get train and test set(Coppied from validation)\n",
    "        trainI = np.random.choice(processed_train_data.shape[0], size = train_size, replace=False)\n",
    "        trainIndex = songClassifyData.index.isin(trainI)\n",
    "        \n",
    "        train_dataset = processed_train_data.iloc[trainIndex]\n",
    "        #rest of index will be used as validation dataset for the model\n",
    "        validation_dataset = processed_train_data.iloc[~trainIndex]\n",
    "        \n",
    "        X_train= train_dataset.drop(columns=['output_label','gesture'])\n",
    "        Y_train = train_dataset['output_label']\n",
    "        \n",
    "        X_validation= validation_dataset.drop(columns=['output_label','gesture'])\n",
    "        Y_validation = validation_dataset['output_label']\n",
    "\n",
    "Note: not implemented here: learn the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49775"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-fold Cross validation\n",
    "Here learn the model and calculate accuracy average. \n",
    "=> not implemented here: learn the model and find the accuracy for each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n-fold cross validation\n",
    "np.random.seed(1)\n",
    "# Divide train and validation set as 80% and 20%\n",
    "number_of_rows = len(processed_train_data.index)\n",
    "train_size = int ( (number_of_rows * 8) / 10)\n",
    "accuracy = np.zeros((50, 200))\n",
    "for i in range(number_of_epoch):\n",
    "        #get 80% randomly choice of trainindex\n",
    "        trainI = np.random.choice(processed_train_data.shape[0], size = train_size, replace=False)\n",
    "        trainIndex = processed_train_data.index.isin(trainI)\n",
    "        \n",
    "        train_dataset = processed_train_data.iloc[trainIndex]\n",
    "        #rest of index will be used as validation dataset for the model\n",
    "        validation_dataset = processed_train_data.iloc[~trainIndex]\n",
    "        \n",
    "        X_train= train_dataset.drop(columns=['output_label','gesture'])\n",
    "        Y_train = train_dataset['output_label']\n",
    "        \n",
    "        X_validation= validation_dataset.drop(columns=['output_label','gesture'])\n",
    "        Y_validation = validation_dataset['output_label']\n",
    "        \n",
    "        #learn the model and find the accuracy for each epoch\n",
    "\n",
    "# find the average of accuracy            \n",
    "average_accuracy = np.mean(accuracy, axis=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test dataset \n",
    "read the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load test dataset input\n",
    "test_data = pd.read_excel(test_data_path, index_col=0, comment='#').dropna().reset_index() \n",
    "processed_test_data= processedData(test_data).drop(columns=['ID'])\n",
    "#X_test = processed_test_data.drop(columns=['output_label','gesture','source','frame'])\n",
    "##Y_test = processed_test_data['output_label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function write predicted output, landmark to the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to write output class on video frame and draw circle and line\n",
    "def WriteImageframe(image_frame, img_x_test_input_array, output_class):\n",
    "        cv2.putText(image_frame, \"Predicted \" + str(output_class), (5, 50),cv2.FONT_HERSHEY_SIMPLEX, 2, (128,0,0), 2)\n",
    "\n",
    "        #draw circle and line\n",
    "        xposroot = img_x_test_input_array[\"palm_root_x\"].iloc[ 0 ] \n",
    "        yposroot = img_x_test_input_array[\"palm_root_y\"].iloc[ 0 ] \n",
    "        root_connection_number = 2\n",
    "        thumb_iteration = 2\n",
    "        other_finger_iteration = 12\n",
    "        other_finger_sub_iteration = 3\n",
    "        for indexval in range(0,80,2):\n",
    "            xpos = img_x_test_input_array.iloc[ 0 , indexval ] \n",
    "            ypos = img_x_test_input_array.iloc[ 0 , indexval + 1 ] \n",
    "            cv2.circle(image_frame,(xpos, ypos), 3, (128,0,50),5)\n",
    "\n",
    "            if thumb_iteration == 0 :\n",
    "                if other_finger_sub_iteration == 0:\n",
    "                    other_finger_sub_iteration = 3\n",
    "                    if other_finger_iteration == 0:\n",
    "                        thumb_iteration = 2\n",
    "                        other_finger_iteration = 12\n",
    "                        root_connection_number += 2\n",
    "                        xposroot = img_x_test_input_array[\"dorsal_root_x\"].iloc[ 0 ] \n",
    "                        yposroot = img_x_test_input_array[\"dorsal_root_y\"].iloc[ 0 ] \n",
    "            if thumb_iteration > 0 and  indexval >= root_connection_number :\n",
    "                xposth1 = img_x_test_input_array.iloc[ 0 , indexval ] \n",
    "                yposth1 = img_x_test_input_array.iloc[ 0 , indexval + 1 ] \n",
    "                xposth2 = img_x_test_input_array.iloc[ 0 , indexval + 2] \n",
    "                yposth2 = img_x_test_input_array.iloc[ 0 , indexval + 3] \n",
    "                thumb_iteration -= 1\n",
    "                if thumb_iteration == 0 :\n",
    "                    root_connection_number += 6\n",
    "                if (xposth1 != 0 and yposth1 != 0):\n",
    "                        if (xposth2 != 0 and yposth2 != 0):\n",
    "                            cv2.line(image_frame, (xposth1, yposth1), (xposth2, yposth2), (128, 0, 10), 1)\n",
    "                if indexval == root_connection_number:\n",
    "                    if (xposroot != 0 and yposroot != 0):\n",
    "                        if (xposth1 != 0 and yposth1 != 0):\n",
    "                            cv2.line(image_frame, (xposroot, yposroot), (xposth1, yposth1), (128, 0, 10), 1)           \n",
    "            else:\n",
    "                if thumb_iteration <= 0 and other_finger_iteration > 0 and other_finger_sub_iteration > 0 and indexval >= root_connection_number:\n",
    "                    xposth1 = img_x_test_input_array.iloc[ 0 , indexval ] \n",
    "                    yposth1 = img_x_test_input_array.iloc[ 0 , indexval + 1 ] \n",
    "                    xposth2 = img_x_test_input_array.iloc[ 0 , indexval + 2 ] \n",
    "                    yposth2 = img_x_test_input_array.iloc[ 0 , indexval + 3 ] \n",
    "                    other_finger_iteration -= 1\n",
    "                    other_finger_sub_iteration -= 1\n",
    "                    if other_finger_sub_iteration == 0 :\n",
    "                        root_connection_number += 8\n",
    "                    if (xposth1 != 0 and yposth1 != 0):\n",
    "                            if (xposth2 != 0 and yposth2 != 0):\n",
    "                                cv2.line(image_frame, (xposth1, yposth1), (xposth2, yposth2), (128, 0, 10), 1)\n",
    "                    if indexval == root_connection_number:\n",
    "                        if (xposroot != 0 and yposroot != 0):\n",
    "                            if (xposth1 != 0 and yposth1 != 0):\n",
    "                                cv2.line(image_frame, (xposroot, yposroot), (xposth1, yposth1), (128, 0, 10), 1)  \n",
    "        return image_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the input video for each frame :  predict and write output to the image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subsystem 1 : Landmark extraction. Get input landmark from image and feed it to the model\n",
    "import os\n",
    "current_working_directory = os.getcwd()\n",
    "for directory_name in directory_names:\n",
    "    for video_name in conditions:\n",
    "        for extension in video_file_extensions:\n",
    "            video_path =directory_name + \"\\\\\" + video_name \n",
    "            #print(current_working_directory + \"\\\\\" + video_path + extension)\n",
    "            input_video = cv2.VideoCapture(r\"\" + video_subdirecory_name + \"\\\\\" + video_path + extension)\n",
    "            output_file_name = r\"\" + video_subdirecory_name + \"\\\\\" + video_path  + \"_predicted.mp4\"\n",
    "            backend = cv2.CAP_ANY\n",
    "            fourcc_code = cv2.VideoWriter_fourcc(*\"H264\")\n",
    "            fps = 24\n",
    "            frame_size = (640, 480)\n",
    "            output_video = cv2.VideoWriter(output_file_name, backend, fourcc_code, fps, frame_size)\n",
    "\n",
    "            ret, frame = input_video.read()\n",
    "            counter = 0\n",
    "            while ret:\n",
    "                ret, frame = input_video.read()\n",
    "                if not ret:\n",
    "                    continue\n",
    "                \n",
    "                data_frame = processed_test_data.loc[processed_test_data.source.isin([directory_name + \"/\" + video_name  + extension]) & processed_test_data.frame.isin([counter])]\n",
    "                \n",
    "                #Input landmark - feed this to prediction model\n",
    "                X_test_input = data_frame.drop(columns=['output_label','gesture','source','frame'])\n",
    "                if X_test_input.empty == False :\n",
    "                    #put output class to the image from predicted model. Now put as on the data\n",
    "                    X_test_output = data_frame['output_label']\n",
    "                    output_frame = WriteImageframe( frame, X_test_input ,str(X_test_output.iloc[ 0 ] ))\n",
    "                else:\n",
    "                    output_frame = frame\n",
    "                output_video.write(output_frame)\n",
    "                counter += 1\n",
    "            input_video.release()\n",
    "            output_video.release()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specialization : Not yet implemented(Do not run) : Choose one specialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specialization 1: CNN to identify hand and predict output(from given video file as input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN Classification model generation\n",
    "# Import of keras model and hidden layers for  convolutional network\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "# Construction of model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (5, 5), activation='relu', input_shape=(120, 320, 1))) \n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu')) \n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "# Configures the model for training\n",
    "model.compile(optimizer='adam', # Optimization routine, which tells the computer how to adjust the parameter values to minimize the loss function.\n",
    "              loss='sparse_categorical_crossentropy', # Loss function, which tells us how bad our predictions are.\n",
    "              metrics=['accuracy']) # List of metrics to be evaluated by the model during training and testing.\n",
    "# Trains the model for a given number of epochs (iterations on a dataset) and validates it.\n",
    "model.fit(X_train, Y_train, epochs=5, batch_size=64, verbose=2, validation_data=(X_test, Y_test))\n",
    "# Save entire model to a HDF5 file\n",
    "model.save('handrecognition_model.h5')\n",
    "test_loss, test_acc = model.evaluate(X_test, Y_test)\n",
    "\n",
    "print('Test accuracy: {:2.2f}%'.format(test_acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specialization 1: Automatically detect hand landmark and provide as input to Model (Random forest or NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take live video as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write predicted output to csv \n",
    "This will have 2 column: frame number and output\n",
    "Read this file for the robot part"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
