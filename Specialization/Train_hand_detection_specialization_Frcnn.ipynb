{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Required package\n",
    "h5py\n",
    "Keras\n",
    "numpy\n",
    "opencv-python\n",
    "sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "#load library\n",
    "import ipywidgets as widgets\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ipywebrtc import CameraStream, ImageRecorder, VideoRecorder\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "import sklearn as sk\n",
    "from sklearn.datasets import make_classification\n",
    "import scikitplot as skplt\n",
    "from sklearn.model_selection import train_test_split # Helps with organizing data for training\n",
    "from sklearn.metrics import confusion_matrix # Helps present results as a confusion-matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "video_landmark_data_path = 'Video Data.xlsx'\n",
    "#Read the directory name\n",
    "video_subdirecory_name = \"videos\"\n",
    "directory_names= [\"102\",\"159\",\"294\",\"441\",\"564\",\"576\",\"609\",\"666\",\"711\",\"723\"]\n",
    "#directory_names= [\"666\"]\n",
    "conditions = [\"open_palm\",\"open_dorsal\",\"fist_palm\",\"fist_dorsal\",\"three_fingers_palm\",\"three_fingers_dorsal\"]\n",
    "#conditions = [\"fist_palm\"]\n",
    "video_file_extensions = [\".mp4\",\".webm\"]\n",
    "columns=['filepath', 'x1',  'y1', 'x2','y2','class_name']\n",
    "hand_detect_tune_param = 30\n",
    "#Note Input of training is all land mark and output column name is output_label which classified fro 0 to 5\n",
    "#write image to path\n",
    "train_image_path = \"train_images_hand_specialization/{}.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process data function.\n",
    "def processedData(data):\n",
    "    #process output data by labeling multiclass 0 to 5 instead of one hot encoding\n",
    "    processing_data = data\n",
    "    processing_data.loc[processing_data.camera_facing_side.isin(['open']) & processing_data.gesture.isin(['palm']), 'camera_facing_side'] = 0\n",
    "    processing_data.loc[processing_data.camera_facing_side.isin(['fist']) & processing_data.gesture.isin(['palm']), 'camera_facing_side'] = 1\n",
    "    processing_data.loc[processing_data.camera_facing_side.isin(['three_fingers']) & processing_data.gesture.isin(['palm']), 'camera_facing_side'] = 2\n",
    "    processing_data.loc[processing_data.camera_facing_side.isin(['open']) & processing_data.gesture.isin(['dorsal']), 'camera_facing_side'] = 3\n",
    "    processing_data.loc[processing_data.camera_facing_side.isin(['fist']) & processing_data.gesture.isin(['dorsal']), 'camera_facing_side'] = 4\n",
    "    processing_data.loc[processing_data.camera_facing_side.isin(['three_fingers']) & processing_data.gesture.isin(['dorsal']), 'camera_facing_side'] = 5\n",
    "\n",
    "    # this will be used later to classify if hand is open or dorsal side fro raw image\n",
    "    processing_data.loc[processing_data.gesture == 'palm', 'gesture'] = 0\n",
    "    processing_data.loc[processing_data.gesture == 'dorsal', 'gesture'] = 1\n",
    "    #Convert all column to integer type\n",
    "    processing_data = processing_data.astype({\"camera_facing_side\": np.int64, \"gesture\": np.int64}) \n",
    "    #rename output column name as label\n",
    "    processing_data=processing_data.rename(columns = {'camera_facing_side':'output_label'})\n",
    "    return processing_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load test dataset from video data input\n",
    "test_data = pd.read_excel(video_landmark_data_path, index_col=0, comment='#').dropna().reset_index() \n",
    "processed_test_data= processedData(test_data).drop(columns=['ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****This function is used to train our model to get the hand area from provided video. Tuning parameter here 20 will get exact hand from image as we have provided landmark. Get xmin, xmax, ymin, ymax from the videodata.xlsx and draw rectangle by tuning left and right how much to increase to detect exact hand.\n",
    "param name : hand_detect_tune_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to write output class on video frame and draw circle and line\n",
    "def Extract_hand_from_train_image(image_frame, img_x_test_input_array, hand_tune_param):\n",
    "        #draw circle and line\n",
    "        dropped_filter_position = []\n",
    "        for indexval in range(0,80,2):\n",
    "            xpos = img_x_test_input_array.iloc[ 0 , indexval ] \n",
    "            ypos = img_x_test_input_array.iloc[ 0 , indexval + 1 ] \n",
    "            #cv2.circle(image_frame,(xpos, ypos), 3, (128,0,50),5)\n",
    "            if xpos > 0 or ypos > 0 :\n",
    "                dropped_filter_position.append(xpos)\n",
    "                dropped_filter_position.append(ypos)\n",
    "        #get the filtered list\n",
    "        xpos_list = dropped_filter_position[::2]\n",
    "        ypos_list = dropped_filter_position[1::2]\n",
    "        #print(xpos_list)\n",
    "        #print(ypos_list)\n",
    "        #print(dropped_filter_position)\n",
    "        X_min =  min(xpos_list)\n",
    "        X_max =  max(xpos_list)\n",
    "        Y_min =  min(ypos_list)\n",
    "        Y_max =  max(ypos_list)\n",
    "        hh = Y_max - Y_min + hand_tune_param\n",
    "        ww = X_max - X_min + hand_tune_param\n",
    "        centerx = int ((X_min + X_max) / 2 )\n",
    "        centery = int ((Y_min + Y_max) / 2 )\n",
    "        #Check if frame goes to out of boundary return 0\n",
    "        im_width = image_frame.shape[1]\n",
    "        im_height = image_frame.shape[0]\n",
    "        X1 = int(centerx - ww/2)\n",
    "        Y1 = int(centery - hh/2)\n",
    "        X2 = int(centerx + ww/2)\n",
    "        Y2 = int(centery + hh/2)\n",
    "        if X1 <= 0 or Y1 <= 0 or X2 >= im_width or Y2 >= im_height :\n",
    "            X1=  -1\n",
    "            X2 = -1\n",
    "            Y1 = -1 \n",
    "            Y2 = -1\n",
    "        #cv2.circle(image_frame,(int(centerx - ww/2), int(centery - hh/2)), 3, (128,0,50),5)\n",
    "        #cv2.circle(image_frame,(int(centerx + ww/2), int(centery + hh/2)), 3, (128,0,50),5)\n",
    "        #else:\n",
    "            #cv2.rectangle(image_frame,\n",
    "                #(int(centerx - ww/2), int(centery - hh/2)),\n",
    "                #(int(centerx + ww/2), int(centery + hh/2)),\n",
    "                #(0,0,255), 2)\n",
    "        \n",
    "        #print( str(X_min) + \"M \" + str(X_max) + \"M \" + str(Y_min)   + \"M \" + str(Y_max))\n",
    "        return image_frame , X1, Y1, X2, Y2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Save all image from video to train_images path and generate a text file with columns=['filepath', 'x1',  'y1', 'x2','y2','class_name'] and here we have only one class called hand and other is not identified. Now we have to just put one class to text file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not run this section if you allready saved the image to train folder and generated text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_data_frame = pd.DataFrame(columns = columns)\n",
    "bad_annotated_data_frame = pd.DataFrame(columns = ['file_name'])\n",
    "\n",
    "import os\n",
    "current_working_directory = os.getcwd()\n",
    "for directory_name in directory_names:\n",
    "    for video_name in conditions:\n",
    "        for extension in video_file_extensions:\n",
    "            video_path =directory_name + \"\\\\\" + video_name \n",
    "            #print(current_working_directory + \"\\\\\" + video_path + extension)\n",
    "            input_video = cv2.VideoCapture(r\"\" + video_subdirecory_name + \"\\\\\" + video_path + extension)\n",
    "            output_file_name = r\"\" + video_subdirecory_name + \"\\\\\" + video_path  + \"_predicted.mp4\"\n",
    "            backend = cv2.CAP_ANY\n",
    "            fourcc_code = cv2.VideoWriter_fourcc(*\"H264\")\n",
    "            fps = 24\n",
    "            frame_size = (640, 480)\n",
    "            #output_video = cv2.VideoWriter(output_file_name, backend, fourcc_code, fps, frame_size)\n",
    "\n",
    "            ret, frame = input_video.read()\n",
    "            counter = 0\n",
    "            while ret:\n",
    "                ret, frame = input_video.read()\n",
    "                if not ret:\n",
    "                    continue\n",
    "                #if counter == 0:\n",
    "                data_frame = processed_test_data.loc[processed_test_data.source.isin([directory_name + \"/\" + video_name  + extension]) & processed_test_data.frame.isin([counter])]\n",
    "\n",
    "                #Input landmark - feed this to prediction model\n",
    "                X_test_input = data_frame.drop(columns=['output_label','gesture','source','frame'])\n",
    "                if X_test_input.empty == False :\n",
    "                    #put output class to the image from predicted model. Now put as on the data\n",
    "                    X_test_output = data_frame['output_label'].iloc[0]\n",
    "                    #print(X_test_output)\n",
    "                    output_frame, x_min, y_min, x_max, y_max = Extract_hand_from_train_image( frame, X_test_input, hand_detect_tune_param)\n",
    "                    #plt.imshow(output_frame)\n",
    "                    fileName =  directory_name + \"_\" + video_name + \"_\" + str(counter)\n",
    "                    if x_min > 0 or y_min > 0 or x_max > 0 or y_max > 0:\n",
    "                        #cv2.imwrite(train_image_path.format(fileName), frame)\n",
    "                        output_data_frame = output_data_frame.append({columns[0] : train_image_path.format(fileName), columns[1] : x_min, columns[2] : y_min, columns[3] : x_max, columns[4] : y_max, columns[5] : str(X_test_output) }, ignore_index=True)\n",
    "                        #output_video.write(output_frame)\n",
    "                    else:\n",
    "                        bad_annotated_data_frame = bad_annotated_data_frame.append({'file_name': fileName}, ignore_index=True)\n",
    "                counter += 1\n",
    "            input_video.release()\n",
    "            #output_video.release()\n",
    "\n",
    "\n",
    "bad_annotated_data_frame.to_csv('bad_annotated_data-fame.csv', index=False)\n",
    "#output_data_frame.to_csv('annotate.txt', header=None, index=None, sep=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate text File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame()\n",
    "data['format'] = output_data_frame['filepath']\n",
    "\n",
    "# add xmin, ymin, xmax, ymax and class as per the format required\n",
    "for i in range(data.shape[0]):\n",
    "    data['format'][i] =  data['format'][i] + ',' + str(output_data_frame['x1'][i]) + ',' + str(output_data_frame['y1'][i]) + ',' + str(output_data_frame['x2'][i]) + ',' + str(output_data_frame['y2'][i]) + ',' + output_data_frame['class_name'][i]\n",
    "\n",
    "data.to_csv('open_three_closed_annotate.txt', header=None, index=None, sep=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train frcnn hand detection and specialization model using terminal\n",
    "parser can be pascal_voc or simple we used simple: Go to the location of train_frcnn.py and run using below command\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python train_frcnn.py -o simple -p open_three_closed_annotate.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
