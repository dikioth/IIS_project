{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "import cv2\n",
    "import numpy as np\n",
    "from ipywebrtc import CameraStream, ImageRecorder, VideoRecorder\n",
    "from src.View import JupyterGUIFactory\n",
    "from src.Controller import StateManager\n",
    "from src.landmark_names import landmark_names\n",
    "# from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [\"open_palm\",\n",
    "             \"open_dorsal\",\n",
    "             \"fist_palm\",\n",
    "             \"fist_dorsal\",\n",
    "             \"three_fingers_palm\",\n",
    "             \"three_fingers_dorsal\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 - Raw Videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Record 6 videos of your hand (5+ seconds), each showing a different gesture (for examples check the instructions for assignment 1):\n",
    "\n",
    "1.\t**open_plam**: The palm side is facing the camera, and the hand is open; the fingers are spread.\n",
    "2.\t**open_dorsal**: The dorsal side (back of the hand) is facing the camera, and the hand is open; the fingers are spread\n",
    "3.\t**fist_palm**: The palm side is facing the camera, and the hand is closed to a fist\n",
    "4.\t**fist_dorsal**: The dorsal side is facing the camera, and the hand is closed to a fist\n",
    "5.\t**three_fingers_palm**: The palm side is facing the camera, and three fingers are spread out: thumb, index, and middle fingers. Ring, and pinky fingers are folded in.\n",
    "6.\t**three_fingers_dorsal**: The dorsal side is facing the camera, and three fingers are spread out: thumb, index, and middle fingers. Ring and pinky fingers are folded in.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important:** While recording the videos move your hand around slowly in the image, move it closer to the camera or further away, or do mild rotations. \n",
    "\n",
    "This is done to create more diverse training data. The hand can be anywhere on the screen, arbitrarily rotated, and at any distance from the camera (as long as it is still identifiable as a hand). The broader the set of examples, the more robust the trained model will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CameraStream(constraints={'facing_mode': 'user', 'audio': False, 'video': {'width': 640, 'height': 480}})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "camera = CameraStream(constraints=\n",
    "                      {'facing_mode': 'user',\n",
    "                       'audio': False,\n",
    "                       'video': { 'width': 640, 'height': 480 }\n",
    "                       })\n",
    "camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "438adb32edda40c081e1b86beadb8137",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VideoRecorder(stream=CameraStream(constraints={'facing_mode': 'user', 'audio': False, 'video': {'width': 640, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "recorder = VideoRecorder(stream=camera)\n",
    "recorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you are happy with the video save it using the name of the condition (bold word in above list)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name=\"three_fingers_dorsal\"\n",
    "with open(file_name+\".webm\", 'wb') as out_file:\n",
    "    out_file.write(recorder.video.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_video = cv2.VideoCapture(\"three_fingers_dorsal.webm\")\n",
    "\n",
    "output_file_name = \"three_fingers_dorsal.mp4\"\n",
    "backend = cv2.CAP_ANY\n",
    "fourcc_code = cv2.VideoWriter_fourcc(*\"H264\")\n",
    "fps = 24\n",
    "frame_size = (640, 480)\n",
    "output_video = cv2.VideoWriter(output_file_name, backend, fourcc_code, fps, frame_size)\n",
    "\n",
    "ret, frame = input_video.read()\n",
    "counter = 0\n",
    "while ret:\n",
    "    ret, frame = input_video.read()\n",
    "    if not ret:\n",
    "        continue\n",
    "    output_video.write(frame)\n",
    "input_video.release()\n",
    "output_video.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional:** If you want the video to be anonymous and your face can be seen in the video, you may anonymize it using the face detection method presented in lab 1. For each detected face, either draw a solid box over that section of the image, or blurr the region using gaussian blur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# place the code for loading, anonymizing, and saving the video here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One you are done with Task 1, release the camera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the video using the name of the condition (bold word in above list). Create a .zip file containing either all 6 original or all 6 anonymized videos. The name of the .zip file must be of the form `<last name>_<first name>_<student number>_videos.zip`, where you replace <last name>, <first name>, and <student number> with your last name, first name, and student number. Submit The .zip file on studentportalen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Optional:** Fill out the consent form to allow us to use the raw videos to create a benchmark dataset for hand gesture recognition and and skeleton tracking from 2D video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 - Annotating the Videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task you will not have to write any code; instead, this notebook provides an annotation tool so that you can work more efficiently. This ensures that everybody has the same format for their annotations and saves you some time in the data processing / cleaning phase.\n",
    "\n",
    "It will still show you the challenges associated with this step, as - for most (real) ML projects - data cleaning and preparation is where you spend the majority of your time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb07506362a14273a18ef81c1f33de12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "state = StateManager(conditions, video_format=\".mp4\")\n",
    "gui = JupyterGUIFactory(state, conditions)\n",
    "# below you will see a figure with the first frame of a condition you recorded\n",
    "# this is NOT the GUI; you will execute the GUI in the next code block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to use this annotation tool:**\n",
    "1. Use the \"Video\" drop down menu to select and jump between videos (progress is preserved) #done\n",
    "2. Use the \"Frames\" slider (timeline) to scrub through the video\n",
    "3. Use the two blue arrow buttons at the bottm left to select the landmark you wish to place\n",
    "4. Click on the image to place the landmark so that it matches the location in the graphic next to it. This does the following:\n",
    "    0. (If you know the concept of keyframing for animation, this is what is happening)\n",
    "    1. A marker for the selected landmark is placed at the position for this frame\n",
    "    2. The marker position becomes a supporting point for the trajectory of the landmark over time (keyframe)\n",
    "    3. Frames that aren't keyframes will have the marker position calculated by interpolating linearly between the two nearest keyframes\n",
    "5. Use the above steps to create markers for all visible landmarks in the first and last frame of the video\n",
    "6. Use the timeline slider to slowly scrub through the timeline and observe the position of the marker change over time\n",
    "7. If the position doesn't match where it is supposed to be (on the respective joint of the hand) do the following:\n",
    "    1. Click on the markers to select it\n",
    "    2. Drag the marker around in the image so that it is in the correct position again\n",
    "    3. move the timeline slider a bit back to check if the marker position follows the hand well, if not repeat these three steps and adjust further\n",
    "8. Before moving to the next video, use the timeline once more to visuall inspect that all the markers follow their intended location.\n",
    "9. Use this method to annotate all videos\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c05adb2decb41f9a5a7a16d1acdb643",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=0, description='Frames:', layout=Layout(width='100%'), max=222),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important:** This step is crucial for good performance of your machine learning model. ML is not black magic, and your model can only ever be as good as the data you use for training. If you are sloppy here, you will not have high performance later. The saying \"trash in, trash out\" is very true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the annotation has been completed for each video, you can generate a .csv file with the marker positions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the two segments below to generate the file in your working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_name = \"Miah\"\n",
    "first_name=\"Shafi\"\n",
    "student_number=9111131356"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "state.createCsv(last_name, first_name, student_number, conditions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important:** Please modify the block above, so that the values match your last name, first name and student number. They will be used to generate the .csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the .csv file to studentportalen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 - Annotated Videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write code that reads the CSV file you just generated into a numpy array. (You will do the same later when you load the data to train a model)\n",
    "\n",
    "Then, load each of the 6 videos from task 1 and perform the following steps (pseudo code):\n",
    "```\n",
    "for each video:\n",
    "    open a new_video file called <video name>_annotated\n",
    "    for each frame in video:\n",
    "        place a label into the top left corner that reads \"annotated\"\n",
    "        get the corresponding landmark positions from the CSV file\n",
    "        draw a circle at each (non-missing) landmark position\n",
    "        for each pair of visible landmarks connected by the skeleton:\n",
    "            draw a line between the two landmarks using cv2.line\n",
    "        write the frame into the new_video\n",
    "    close the new_video\n",
    "```\n",
    "\n",
    "Name the new resulting videos `<video name>_annotated` where `<video name>` is the name of the pose, e.g., `fist_palm_annotated` for the annotated video of the fist facing the camera palmside. Then, create a .zip file named \n",
    "`<last name>_<first name>_<student number>_videos_annotated.zip`, replacing the tags as you did in task 1, and upload it to studentportalen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [\"open_palm\",\n",
    "             \"open_dorsal\",\n",
    "             \"fist_palm\",\n",
    "             \"fist_dorsal\",\n",
    "             \"three_fingers_palm\",\n",
    "             \"three_fingers_dorsal\"]\n",
    "#Read csv Data\n",
    "import csv\n",
    "data_path = 'Miah_Shafi_9111131356_annotations.csv'\n",
    "with open(data_path, 'r') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    headers = next(reader)\n",
    "    csv_data = np.array(list(reader)).astype(str)\n",
    "#End read csv data\n",
    "#Create video\n",
    "for video_name in conditions:\n",
    "    input_video = cv2.VideoCapture(video_name + \".mp4\")\n",
    "    output_file_name = video_name + \"_annotated.mp4\"\n",
    "    backend = cv2.CAP_ANY\n",
    "    fourcc_code = cv2.VideoWriter_fourcc(*\"H264\")\n",
    "    fps = 24\n",
    "    frame_size = (640, 480)\n",
    "    output_video = cv2.VideoWriter(output_file_name, backend, fourcc_code, fps, frame_size)\n",
    "\n",
    "    ret, frame = input_video.read()\n",
    "    counter = 0\n",
    "    while ret:\n",
    "        ret, frame = input_video.read()\n",
    "        if not ret:\n",
    "            continue\n",
    "            \n",
    "        data_frame = csv_data[np.where((csv_data[:,0] == video_name + '.mp4') * (csv_data[:,1] == str(counter)))]\n",
    "        cv2.putText(frame, \"annotated\", (5, 50),cv2.FONT_HERSHEY_SIMPLEX, 2, (128,0,0), 2)\n",
    "        #print a circle\n",
    "        for indexval in range(4,84,2):\n",
    "            xpos =int(float(data_frame[0][indexval]))\n",
    "            ypos = int(float(data_frame[0][indexval + 1]))\n",
    "            cv2.circle(frame,(xpos, ypos), 3, (128,0,50),5)\n",
    "        #palm draw line\n",
    "        #bura angul\n",
    "        xposroot =int(float(data_frame[0][4]))\n",
    "        yposroot = int(float(data_frame[0][5]))\n",
    "        number = 6\n",
    "        for indexval in range(6,9,2):\n",
    "            xposth1 =int(float(data_frame[0][indexval]))\n",
    "            yposth1 = int(float(data_frame[0][indexval + 1]))\n",
    "            xposth2 =int(float(data_frame[0][indexval+2]))\n",
    "            yposth2 = int(float(data_frame[0][indexval + 3]))\n",
    "            if (xposth1 != 0 and yposth1 != 0):\n",
    "                        if (xposth2 != 0 and yposth2 != 0):\n",
    "                            cv2.line(frame, (xposth1, yposth1), (xposth2, yposth2), (128, 0, 10), 1)\n",
    "            if indexval == number :\n",
    "                number += 6\n",
    "                if (xposroot != 0 and yposroot != 0):\n",
    "                    if (xposth1 != 0 and yposth1 != 0):\n",
    "                        cv2.line(frame, (xposroot, yposroot), (xposth1, yposth1), (128, 0, 10), 1)\n",
    "        #run this 4 time\n",
    "        for i in range(0,4,1):\n",
    "            for indexval in range(number,number + 6,2):\n",
    "                xposth1 =int(float(data_frame[0][indexval]))\n",
    "                yposth1 = int(float(data_frame[0][indexval + 1]))\n",
    "                xposth2 =int(float(data_frame[0][indexval+2]))\n",
    "                yposth2 = int(float(data_frame[0][indexval + 3]))\n",
    "                if (xposth1 != 0 and yposth1 != 0):\n",
    "                        if (xposth2 != 0 and yposth2 != 0):\n",
    "                            cv2.line(frame, (xposth1, yposth1), (xposth2, yposth2), (128, 0, 10), 1)\n",
    "                if indexval == number :\n",
    "                    number += 8\n",
    "                    if (xposroot != 0 and yposroot != 0):\n",
    "                        if (xposth1 != 0 and yposth1 != 0):\n",
    "                            cv2.line(frame, (xposroot, yposroot), (xposth1, yposth1), (128, 0, 10), 1)\n",
    "        #now it will come to dorsal side\n",
    "        xposroot =int(float(data_frame[0][number]))\n",
    "        yposroot = int(float(data_frame[0][number + 1]))\n",
    "        number += 2\n",
    "        for indexval in range(number,number + 4,2):\n",
    "            xposth1 =int(float(data_frame[0][indexval]))\n",
    "            yposth1 = int(float(data_frame[0][indexval + 1]))\n",
    "            xposth2 =int(float(data_frame[0][indexval+2]))\n",
    "            yposth2 = int(float(data_frame[0][indexval + 3]))\n",
    "            if (xposth1 != 0 and yposth1 != 0):\n",
    "                        if (xposth2 != 0 and yposth2 != 0):\n",
    "                            cv2.line(frame, (xposth1, yposth1), (xposth2, yposth2), (128, 0, 10), 1)\n",
    "            if indexval == number :\n",
    "                number += 6\n",
    "                if (xposroot != 0 and yposroot != 0):\n",
    "                    if (xposth1 != 0 and yposth1 != 0):\n",
    "                        cv2.line(frame, (xposroot, yposroot), (xposth1, yposth1), (128, 0, 10), 1)\n",
    "        #run this 4 time\n",
    "        for i in range(0,4,1):\n",
    "            for indexval in range(number,number + 6,2):\n",
    "                xposth1 =int(float(data_frame[0][indexval]))\n",
    "                yposth1 = int(float(data_frame[0][indexval + 1]))\n",
    "                xposth2 =int(float(data_frame[0][indexval+2]))\n",
    "                yposth2 = int(float(data_frame[0][indexval + 3]))\n",
    "                if (xposth1 != 0 and yposth1 != 0):\n",
    "                        if (xposth2 != 0 and yposth2 != 0):\n",
    "                            cv2.line(frame, (xposth1, yposth1), (xposth2, yposth2), (128, 0, 10), 1)\n",
    "                if indexval == number :\n",
    "                    number += 8\n",
    "                    if (xposroot != 0 and yposroot != 0):\n",
    "                        if (xposth1 != 0 and yposth1 != 0):\n",
    "                             cv2.line(frame, (xposroot, yposroot), (xposth1, yposth1), (128, 0, 10), 1)\n",
    "        output_video.write(frame)\n",
    "        counter += 1\n",
    "    input_video.release()\n",
    "    output_video.release()\n",
    "\n",
    "#\n",
    "#data_specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,10,1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
