{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "from sklearn.datasets import make_classification\n",
    "import scikitplot as skplt\n",
    "import pandas as pd\n",
    "import copy\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the CSV data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "# PART I: Preparing Dataset.#\n",
    "# @Elvis                    #\n",
    "#############################\n",
    "# The following code assumes that: \n",
    "# 1: .CSV files contains all hand gestures.\n",
    "# 2: frame column exits. (This will be handled here).\n",
    "# 3: 20 landmarks for each side of hand. (40 in total).\n",
    "\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Macros\n",
    "LANDMARKS_NUM            = 80\n",
    "LANDMARS_OFFSET          = 4\n",
    "INDEX_FRAME              = 1\n",
    "INDEX_CAMERA_FACING_SIDE = 2\n",
    "INDEX_SOURCE             = 0\n",
    "DATASET_PATH             = 'dataset/subsystem_2/Dataset_subsystem_2.csv'\n",
    "# Pre processing params\n",
    "shuffle    = True         # Shuffle data before using.\n",
    "dropout    = False        # Dropout regularization.\n",
    "gradcheck  = False        # Gradient checking. \n",
    "moredata   = False        # Extend dataset adding modifications of data.\n",
    "test_size  = 0.10         # Train, dev, test percentage. \n",
    "\n",
    "# Conditions\n",
    "conds = [\"open_palm\", \"open_dorsal\", \"fist_palm\", \"fist_dorsal\", \"three_fingers_palm\", \"three_fingers_dorsal\"]\n",
    "\n",
    "# Loading final dataset!TODO: Add script to check if correct imported.\n",
    "dbase = pd.read_csv(DATASET_PATH, sep=\",\")\n",
    "\n",
    "#To put the number of the class in the ID column\n",
    "for i in range(len(dbase)):\n",
    "    string = dbase.iloc[i][\"camera_facing_side\"] + '_' + dbase.iloc[i][\"gesture\"]\n",
    "    dbase.at[i,\"ID\"] = conds.index(string)\n",
    "    \n",
    "#to select relevant data for classification\n",
    "\n",
    "Yfeatures = dbase.columns[0]\n",
    "Xfeatures = dbase.columns[4:]\n",
    "print(f'Shape imported data: {dbase.shape}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xfeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare X and Y data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separting data and labels\n",
    "X = dbase[Xfeatures].to_numpy(dtype=np.float32)\n",
    "Y = dbase[Yfeatures].to_numpy(dtype=np.float32)\n",
    "\n",
    "# Display random hand gestures.\n",
    "#!TODO: finish this.\n",
    "\n",
    "# One hot encoding using Sklearn\n",
    "onehot_encoder = sk.preprocessing.OneHotEncoder(dtype=np.float32)\n",
    "onehot_encoder.fit(Y.reshape((-1,1))) # Encoding is [open, first, three_fingers, dorsal, palm]\n",
    "\n",
    "Y_encoded = onehot_encoder.transform(Y.reshape((-1,1))).toarray()\n",
    "# onehot_encoded.shape\n",
    "# Use the line below to invert element of index'IDX'.\n",
    "#inverted = label_encoder.inverse_transform([np.argmax(onehot_encoded[IDX, :])])\n",
    "\n",
    "# Split data into train and test sets. Shuffe data if param enabled.\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = test_size, shuffle=shuffle)\n",
    "# print(f'\\nShape of training set\\t: {X_train.shape}\\nShape of test set\\t: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use  One-Hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_encoded = onehot_encoder.transform(Y_train.reshape((-1,1))).toarray()\n",
    "Y_test_encoded = onehot_encoder.transform(Y_test.reshape((-1,1))).toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this snippet will dissapear at some point in the future\n",
    "# it exists because I found two bugs in KerasClassifier while writing\n",
    "# this notebook. \n",
    "# Issues are raised in the tensorflow repo and this should be fixed soon(TM)\n",
    "\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier \n",
    "\n",
    "class KerasClassifier_Patched(KerasClassifier):\n",
    "    # bugfix: classifier doesn't declare that it is a classifier\n",
    "    # in the Scikit learn API\n",
    "    _estimator_type = \"classifier\"\n",
    "    \n",
    "    # bugfix: the current wrapper does not work with HotOne encoded\n",
    "    # labels\n",
    "    # this is only a fix in the specific case of this notebook,\n",
    "    # not a general one\n",
    "    def score(self, x, y, **kwargs):\n",
    "        _, accuracy = self.model.evaluate(x,y, verbose=0, **kwargs)\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "def setupModel(): #very similar to the one from Lab 2\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Input(shape=(80)))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(6))\n",
    "    model.add(tf.keras.layers.Softmax())\n",
    "    model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=1e-4),\n",
    "                 loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "                 metrics=[tf.keras.metrics.CategoricalAccuracy()])\n",
    "    return model\n",
    "\n",
    "final_model = KerasClassifier_Patched(build_fn=setupModel,\n",
    "                               epochs=3,\n",
    "                               batch_size=50,\n",
    "                               verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Model training\n",
    "final_model.fit(X_train,Y_train_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#testing model accuracy\n",
    "test_accuracy = final_model.score(X_test, Y_test_encoded)\n",
    "test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [15,15]\n",
    "sk.metrics.plot_confusion_matrix(final_model,X_test,Y_test,normalize=\"pred\",display_labels=conds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reminder: conds = [\"open_palm\", \"open_dorsal\", \"fist_palm\", \"fist_dorsal\", \"three_fingers_palm\", \"three_fingers_dorsal\"]\n",
    "X_test\n",
    "final_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "folds = 3 # number of chunks to split the dataset into\n",
    "scores = cross_val_score(final_model, X, Y_encoded, cv=folds)\n",
    "print(f\"Accuracy: {scores.mean():.2f} Standard Deviation: {scores.std():0.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save/load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "final_model_json = final_model.model.to_json()\n",
    "with open(\"final_model.json\", \"w\") as json_file :\n",
    "    json_file.write(final_model_json)\n",
    "\n",
    "final_model.model.save_weights(\"final_model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "\n",
    "final_model.model.save('final_model.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the model\n",
    "model = tf.keras.models.load_model(\"final_model.model\")\n",
    "input_data = X_test #your image path\n",
    "prediction = model.predict(input_data)\n",
    "prediction = np.argmax(prediction,axis=1)\n",
    "prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
