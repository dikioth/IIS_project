{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "from sklearn.datasets import make_classification\n",
    "import scikitplot as skplt\n",
    "import pandas as pd\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row length should be 81 (80 landmarks + label). Got: 81\n",
      "A total of 4 datasets of shape [(1323, 81), (1710, 81), (703, 81), (1246, 81)] datasets where added.\n",
      "The total concatenated dataset is: (4982, 81)\n",
      "\n",
      "Shape of training set\t: (4483, 80)\n",
      "Shape of test set\t: (499, 80)\n"
     ]
    }
   ],
   "source": [
    "#############################\n",
    "# PART I: Preparing Dataset.#\n",
    "# @Elvis                    #\n",
    "#############################\n",
    "# The following code assumes that the \n",
    "# 1: .CSV files contains all hand gestures.\n",
    "# 2: frame column exits. (This will be handled here).\n",
    "# 3: 20 landmarks for each side of hand. (40 in total).\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Macros\n",
    "LANDMARKS_NUM            = 80\n",
    "LANDMARS_OFFSET          = 1\n",
    "INDEX_FRAME              = 1\n",
    "INDEX_CAMERA_FACING_SIDE = 2\n",
    "\n",
    "# Pre processing params\n",
    "shuffle    = True         # Shuffle data before using.\n",
    "dropout    = False        # Dropout regularization.\n",
    "gradcheck  = False        # Gradient checking. \n",
    "moredata   = False        # Extend dataset to \n",
    "test_size  = 0.10         # Train, dev, test percentage. \n",
    "\n",
    "# Conditions\n",
    "# conds = [\"open_palm\", \"open_dorsal\", \"fist_palm\", \"fist_dorsal\", \"three_fingers_palm\", \"three_fingers_dorsal\"]\n",
    "\n",
    "# Loading all datasets (All .csv files) !TODO: Add script to check if correct imported.\n",
    "paths = [os.path.join(r, f) for r,_,fs in os.walk('dataset') for f in fs if f.endswith('annotations.csv')]\n",
    "dbase = [pd.read_table(path, sep=\",\", usecols = [n for n in range(LANDMARKS_NUM + 3) if n != INDEX_FRAME and n!= INDEX_CAMERA_FACING_SIDE]) for path in paths]\n",
    "print(f'Row length should be 81 (80 landmarks + label). Got: {dbase[0].to_numpy().shape[1]}')\n",
    "\n",
    "# Converting data into Numpy array\n",
    "data = np.array([dbase[i].to_numpy() for i in range(len(dbase))]) # Dataset.\n",
    "print(f'A total of {len(dbase)} datasets of shape {[d.shape for d in data]} datasets where added.')\n",
    "data = np.concatenate(data) # Stack all datasets into one array.\n",
    "print(f'The total concatenated dataset is: {data.shape}')\n",
    "\n",
    "# Separting data and labels\n",
    "X = data[:, LANDMARS_OFFSET:]\n",
    "Y = data[:, 0]\n",
    "\n",
    "# One hot encoding using Sklearn\n",
    "label_encoder = LabelEncoder() # integer encode\n",
    "integer_encoded = label_encoder.fit_transform(np.array(Y))\n",
    "onehot_encoder = OneHotEncoder(sparse = False) # Binary encode\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "# print(onehot_encoded)\n",
    "\n",
    "# Use the line below to invert element of index'IDX'.\n",
    "#inverted = label_encoder.inverse_transform([np.argmax(onehot_encoded[IDX, :])])\n",
    "\n",
    "# Split data into train and test sets. Shuffe data if param enabled.\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, onehot_encoded, test_size = test_size, shuffle=shuffle)\n",
    "print(f'\\nShape of training set\\t: {X_train.shape}\\nShape of test set\\t: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categories='auto', drop=None, dtype=<class 'numpy.float32'>,\n",
       "              handle_unknown='error', sparse=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "landmarks_0 = pd.read_table(\"dataset/Clement/ribes_clement_266_annotations.csv\",sep=\",\")\n",
    "landmarks_1 = copy.copy(landmarks_0)\n",
    "del landmarks_1['frame']\n",
    "del landmarks_1['camera_facing_side']\n",
    "# landmarks_1\n",
    "\n",
    "landmarks_2 = copy.copy(landmarks_1)\n",
    "for i in range(len(landmarks_2)):\n",
    "    label = landmarks_2.iloc[i][\"source\"]\n",
    "    landmarks_2.at[i,'source'] = label[:-5]\n",
    "# landmarks_2\n",
    "\n",
    "landmarks_3 = copy.copy(landmarks_2)\n",
    "classes = [\"open_palm\",\n",
    "             \"open_dorsal\",\n",
    "             \"fist_palm\",\n",
    "             \"fist_dorsal\",\n",
    "             \"three_fingers_palm\",\n",
    "             \"three_fingers_dorsal\"]\n",
    "\n",
    "for i in range(len(landmarks_3)):\n",
    "    current_class = landmarks_3.at[i,'source']\n",
    "    landmarks_3.at[i,'source'] = classes.index(current_class)\n",
    "landmarks_3\n",
    "\n",
    "landmarks_4 = landmarks_3.to_numpy(dtype=np.float32)\n",
    "landmarks_4\n",
    "\n",
    "landmarks_5 = np.copy(landmarks_4)\n",
    "np.random.shuffle(landmarks_5)\n",
    "landmarks_5[:700].shape\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = landmarks_5[:600,1:], landmarks_5[600:700,1:], landmarks_5[:600,0], landmarks_5[600:700,0]\n",
    "encoder = sk.preprocessing.OneHotEncoder(dtype=np.float32)\n",
    "encoder.fit(Y_train.reshape((-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 0., 0., 1., 0., 3., 4., 1., 4., 3., 2., 0., 3., 2., 1., 0., 1.,\n",
       "       0., 2., 3., 5., 1., 2., 0., 1., 5., 2., 4., 3., 1., 1., 0., 1., 3.,\n",
       "       2., 5., 4., 1., 5., 2., 2., 2., 1., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 1., 3., 3., 2., 0., 4., 5., 3., 2., 1., 4., 1., 5., 2., 3.,\n",
       "       1., 0., 4., 1., 5., 5., 4., 3., 3., 1., 5., 0., 2., 1., 0., 0., 3.,\n",
       "       0., 1., 2., 3., 2., 3., 4., 3., 2., 2., 0., 4., 0., 0., 4.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "Y_train_encoded=encoder.transform(Y_train.reshape((-1,1))).toarray()\n",
    "Y_test_encoded=encoder.transform(Y_test.reshape((-1,1))).toarray()\n",
    "print(Y_test_encoded[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-d964a3c72a39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_train_encoded\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_train_encoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_test_encoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train,Y_train_encoded))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test,Y_test_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group lines in batches\n",
    "BATCH_SIZE = 50\n",
    "train_dataset_batched = train_dataset.batch(BATCH_SIZE)\n",
    "test_dataset_batched = test_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this snippet will dissapear at some point in the future\n",
    "# it exists because I found two bugs in KerasClassifier while writing\n",
    "# this notebook. \n",
    "# Issues are raised in the tensorflow repo and this should be fixed soon(TM)\n",
    "\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier \n",
    "\n",
    "class KerasClassifier_Patched(KerasClassifier):\n",
    "    # bugfix: classifier doesn't declare that it is a classifier\n",
    "    # in the Scikit learn API\n",
    "    _estimator_type = \"classifier\"\n",
    "    \n",
    "    # bugfix: the current wrapper does not work with HotOne encoded\n",
    "    # labels\n",
    "    # this is only a fix in the specific case of this notebook,\n",
    "    # not a general one\n",
    "    def score(self, x, y, **kwargs):\n",
    "        _, accuracy = self.model.evaluate(x,y, verbose=0, **kwargs)\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "def setupModel():\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Input(shape=(80)))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(6))\n",
    "    model.add(tf.keras.layers.Softmax())\n",
    "    model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=1e-4),\n",
    "                 loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "                 metrics=[tf.keras.metrics.CategoricalAccuracy()])\n",
    "    return model\n",
    "\n",
    "mlp_model = KerasClassifier_Patched(build_fn=setupModel,\n",
    "                               epochs=5,\n",
    "                               batch_size=50,\n",
    "                               verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600, 80)\n",
      "(600, 6)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 600 samples\n",
      "Epoch 1/5\n",
      "600/600 [==============================] - 0s 271us/sample - loss: 68.0428 - categorical_accuracy: 0.6783\n",
      "Epoch 2/5\n",
      "600/600 [==============================] - 0s 47us/sample - loss: 8.1237e-07 - categorical_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "600/600 [==============================] - 0s 55us/sample - loss: 8.0979e-07 - categorical_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "600/600 [==============================] - 0s 51us/sample - loss: 8.0701e-07 - categorical_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "600/600 [==============================] - 0s 31us/sample - loss: 8.0442e-07 - categorical_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe348348b50>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_model.fit(X_train,Y_train_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = mlp_model.score(X_test, Y_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
